{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18jg9Sa2IiZ2BMKbRW3NEQwxqxb9n3DH0",
      "authorship_tag": "ABX9TyMUYPEBH6l6FTU9tFpgLXhA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbonato/cnnTestBench/blob/main/pauNaJaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBoqo2tH2d6g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v9NgXTUH3Z0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Dataset WISDM**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IVtz1oB_0MmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOAdb2nepkTb",
        "outputId": "1f8535c4-e1c4-4a5a-e886-253a7d0b5a78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time windows\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "# Function to create time windows\n",
        "def create_time_windows(data, labels, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])  # Select window of data\n",
        "        y.append(labels[i + (window_size-1)])  # Label is from the last element of the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to save windowed data for your C++ program\n",
        "def save_windowed_data_to_dat(X, y, file_path):\n",
        "    \"\"\"Saves the windowed data to a .dat file.\"\"\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        for i in range(X.shape[0]):\n",
        "            feature_vector = X[i].flatten()\n",
        "            feature_str = \",\".join(map(str, feature_vector))\n",
        "            label = y[i]\n",
        "            f.write(f\"{{{{{feature_str}}},{label}}}\\n\")\n",
        "    print(f\"Successfully saved {X.shape[0]} samples to {file_path}\")\n",
        "\n",
        "# Updated data loading function\n",
        "def load_data(file_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Use regular expression to match the pattern {{feature_vector}, label}\n",
        "            match = re.match(r\"\\{\\{([0-9.,-]+)\\},\\s*(\\d+)\\}\", line.strip())\n",
        "\n",
        "            if match:\n",
        "                # Extract feature vector and label\n",
        "                feature_str = match.group(1)  # The feature string \"8.24,-2.11,3.87\"\n",
        "                label = int(match.group(2))  # The label \"4\"\n",
        "\n",
        "                # Convert the feature string to a list of floats\n",
        "                feature_vector = list(map(float, feature_str.split(',')))\n",
        "\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load train and test data\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/WISDM/HAR-Dataset/train.dat'  # Adjust path to your file\n",
        "test_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/WISDM/HAR-Dataset/test.dat'  # Adjust path to your file\n",
        "\n",
        "# Define the window size\n",
        "window_size = 10\n",
        "\n",
        "# Create time windows\n",
        "X_train, y_train = load_data(train_data_file)\n",
        "X_train, y_train = create_time_windows(X_train, y_train, window_size)\n",
        "\n",
        "X_test, y_test = load_data(test_data_file)\n",
        "X_test, y_test = create_time_windows(X_test, y_test, window_size)\n",
        "\n",
        "# --- 2. START: SAVE DATA FOR C++ TRAINER ---\n",
        "# Define where you want to save the new files\n",
        "output_path_train = 'train_windowed.dat' # You can change this path\n",
        "output_path_test = 'test_windowed.dat'   # You can change this path\n",
        "\n",
        "# Call the save function\n",
        "save_windowed_data_to_dat(X_train, y_train, output_path_train)\n",
        "save_windowed_data_to_dat(X_test, y_test, output_path_test)\n",
        "# --- END: SAVE DATA FOR C++ TRAINER ---\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)  # For classification (long type for labels)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yh6d-g0vBXZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b292640b-1785-4a33-e045-b072b0460a83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 738432 samples to train_windowed.dat\n",
            "Successfully saved 316462 samples to test_windowed.dat\n",
            "First feature vector in X_train:\n",
            "[[-0.6946377  12.680544    0.50395286]\n",
            " [ 5.012288   11.264028    0.95342433]\n",
            " [ 4.903325   10.882658   -0.08172209]\n",
            " [-0.61291564 18.496431    3.0237172 ]\n",
            " [-1.1849703  12.108489    7.205164  ]\n",
            " [ 1.3756552  -2.4925237  -6.510526  ]\n",
            " [-0.61291564 10.56939     5.706926  ]\n",
            " [-0.50395286 13.947236    7.0553403 ]\n",
            " [-8.430995   11.413852    5.134871  ]\n",
            " [ 0.95342433  1.3756552   1.6480621 ]]\n",
            "First label in y_train:\n",
            "0\n",
            "First feature vector in X_train:\n",
            "tensor([[-0.6946, 12.6805,  0.5040],\n",
            "        [ 5.0123, 11.2640,  0.9534],\n",
            "        [ 4.9033, 10.8827, -0.0817],\n",
            "        [-0.6129, 18.4964,  3.0237],\n",
            "        [-1.1850, 12.1085,  7.2052],\n",
            "        [ 1.3757, -2.4925, -6.5105],\n",
            "        [-0.6129, 10.5694,  5.7069],\n",
            "        [-0.5040, 13.9472,  7.0553],\n",
            "        [-8.4310, 11.4139,  5.1349],\n",
            "        [ 0.9534,  1.3757,  1.6481]])\n",
            "First label in y_train:\n",
            "tensor(0)\n",
            "X_train shape: torch.Size([738432, 10, 3]), y_train shape: torch.Size([738432])\n",
            "X_test shape: torch.Size([316462, 10, 3]), y_test shape: torch.Size([316462])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "        #self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the input size for the fully connected layer based on the output size of conv1\n",
        "        self.fc1 = nn.Linear(16 * (X_train.shape[1] // 2), 64)  # Flattened size after pooling\n",
        "        self.fc2 = nn.Linear(64, 6)  # Assuming 6 classes for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Add channel dimension (change shape to [batch_size, channels, seq_length])\n",
        "\n",
        "        #x = torch.relu(self.conv1(x))  # Apply conv1 and pooling\n",
        "        #x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 and pooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 and pooling\n",
        "\n",
        "        x = x.view(-1, 16 * (x.shape[2]))  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Apply first fully connected layer\n",
        "        x = self.fc2(x)  # Output layer (no activation since we'll apply softmax in loss)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Loss function (cross-entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "g2wLGd2VBRmq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 3  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "id": "tGh-L7AvBg3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3cf9d2-d7c9-43ea-ed22-05b0e14259a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4274, Accuracy: 0.8497\n",
            "Epoch 2/3, Loss: 0.3366, Accuracy: 0.8841\n",
            "Epoch 3/3, Loss: 0.3117, Accuracy: 0.8936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_preds = (predicted == y_test).sum().item()\n",
        "    accuracy = correct_preds / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIhh3n6iBlDB",
        "outputId": "db7c5010-2820-4ec7-aafe-caefc23acee9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'simple_cnn_model.pth')\n",
        "\n",
        "# Load the model (if needed)\n",
        "model = SimpleCNN()\n",
        "model.load_state_dict(torch.load('simple_cnn_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b3hDlz-mBoIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Dataset PAMAP2**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aB498o73z9yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove columns heartrate and temp of the three IMUs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(train_data_file)\n",
        "\n",
        "# Remove columns\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df = df.drop(df.columns[11], axis=1)\n",
        "df = df.drop(df.columns[20], axis=1)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# Print a message to confirm that the file was saved\n",
        "print(f\"Updated CSV saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "ybJnxdyB7hhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the activity names in the first column with their corresponding IDs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the modified CSV\n",
        "modified_train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv'\n",
        "\n",
        "# Load the modified CSV into a DataFrame without assuming a header\n",
        "df = pd.read_csv(modified_train_data_file, header=None)\n",
        "\n",
        "# Print the first few rows to check the content\n",
        "print(\"Original DataFrame first few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Create a dictionary to map activity names to activity IDs\n",
        "activity_map = {\n",
        "    'lying': 1,\n",
        "    'sitting': 2,\n",
        "    'standing': 3,\n",
        "    'walking': 4,\n",
        "    'running': 5,\n",
        "    'cycling': 6,\n",
        "    'nordic_walking': 7,\n",
        "    'watching_TV': 9,\n",
        "    'computer_work': 10,\n",
        "    'car_driving': 11,\n",
        "    'ascending_stairs': 12,\n",
        "    'descending_stairs': 13,\n",
        "    'vacuum_cleaning': 16,\n",
        "    'ironing': 17,\n",
        "    'folding_laundry': 18,\n",
        "    'house_cleaning': 19,\n",
        "    'playing_soccer': 20,\n",
        "    'rope_jumping': 24,\n",
        "    'other': 0\n",
        "}\n",
        "\n",
        "# Replace the activity names in the first column with their corresponding IDs\n",
        "df.iloc[:, 0] = df.iloc[:, 0].map(activity_map)\n",
        "\n",
        "# Print the first few rows after the transformation to ensure it's working\n",
        "print(\"\\nDataFrame after replacing activity names with IDs:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv'\n",
        "df.to_csv(output_file, index=False, header=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"Updated CSV with activity IDs saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "zuGjD4IYE8mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split CSV in train (excluding user 5) and test (only user 5) csv files\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the CSV file containing activity IDs\n",
        "train_data_file_with_ids = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame without assuming a header\n",
        "df = pd.read_csv(train_data_file_with_ids, header=None)\n",
        "\n",
        "# Print the first few rows to check the content\n",
        "print(\"Original DataFrame first few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Filter rows where the user ID is 5 and save them as test_pamap2.csv\n",
        "test_df = df[df.iloc[:, 1] == 5]\n",
        "test_df.to_csv('/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.csv', index=False, header=False)\n",
        "\n",
        "# Filter rows where the user ID is not 5 (excluding) and save them as train_pamap2.csv\n",
        "train_df = df[df.iloc[:, 1] != 5]\n",
        "train_df.to_csv('/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.csv', index=False, header=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Data has been split into train_pamap2.csv (excluding user ID 5) and test_pamap2.csv (user ID 5 only).\")\n"
      ],
      "metadata": {
        "id": "ColCz8mlHdJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert .csv to .dat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "train_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.csv'\n",
        "test_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.csv'\n",
        "\n",
        "# Function to convert rows into the required format and save as .dat\n",
        "def convert_to_dat(input_file, output_file):\n",
        "    # Load the CSV file without a header\n",
        "    df = pd.read_csv(input_file, header=None)\n",
        "\n",
        "    # Open the output .dat file to write\n",
        "    with open(output_file, 'w') as file:\n",
        "        for _, row in df.iterrows():\n",
        "            # Convert row into the required format: {{column 2, column 3, column 4, ...}, column 0}\n",
        "            row_data = \"{{\" + \",\".join(map(str, row[2:])) + \"},\" + str(int(row[0])) + \"}\\n\"\n",
        "            file.write(row_data)\n",
        "\n",
        "# Convert train_pamap2.csv to train_pamap2.dat\n",
        "convert_to_dat(train_file, '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.dat')\n",
        "\n",
        "# Convert test_pamap2.csv to test_pamap2.dat\n",
        "convert_to_dat(test_file, '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.dat')\n",
        "\n",
        "print(\"Conversion to .dat files is complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cncd9etsQcGQ",
        "outputId": "30a72bcb-8efa-4113-b9c8-d0768d1e4d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion to .dat files is complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time windows\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "# Function to create time windows\n",
        "def create_time_windows(data, labels, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])  # Select window of data\n",
        "        y.append(labels[i + (window_size-1)])  # Label is from the last element of the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Updated data loading function\n",
        "def load_data(file_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Use regular expression to match the pattern {{feature_vector}, label}\n",
        "            match = re.match(r\"\\{\\{([0-9.,-]+)\\},\\s*(\\d+)\\}\", line.strip())\n",
        "\n",
        "            if match:\n",
        "                # Extract feature vector and label\n",
        "                feature_str = match.group(1)  # The feature string \"8.24,-2.11,3.87\"\n",
        "                label = int(match.group(2))  # The label \"4\"\n",
        "\n",
        "                # Convert the feature string to a list of floats\n",
        "                feature_vector = list(map(float, feature_str.split(',')))\n",
        "\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load train and test data\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.dat'  # Adjust path to your file\n",
        "test_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.dat'  # Adjust path to your file\n",
        "\n",
        "print(\"File loaded:\")\n",
        "\n",
        "# Define the window size\n",
        "window_size = 10\n",
        "\n",
        "# Create time windows\n",
        "X_train, y_train = load_data(train_data_file)\n",
        "X_train, y_train = create_time_windows(X_train, y_train, window_size)\n",
        "\n",
        "X_test, y_test = load_data(test_data_file)\n",
        "X_test, y_test = create_time_windows(X_test, y_test, window_size)\n",
        "\n",
        "print(\"Time window created:\")\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"Second feature vector in X_train:\")\n",
        "print(X_train[1])  # First row (first feature vector)\n",
        "\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "print(\"Second label in y_train:\")\n",
        "print(y_train[1])  # First label\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)  # For classification (long type for labels)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# # Print the first feature vector and label\n",
        "# print(\"First feature vector in X_train:\")\n",
        "# print(X_train[0])  # First row (first feature vector)\n",
        "# print(\"First label in y_train:\")\n",
        "# print(y_train[0])  # First label\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "2cslxq8BXNTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=27, out_channels=27, kernel_size=3, padding=1)  # input channels = number of features\n",
        "        self.conv2 = nn.Conv1d(in_channels=27, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the input size for the fully connected layer based on the output size of conv1\n",
        "        self.fc1 = nn.Linear(16 * (X_train.shape[1] // 2), 64)  # Flattened size after pooling\n",
        "        self.fc2 = nn.Linear(64, 25)  # Assuming 25 classes for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Add channel dimension (change shape to [batch_size, channels, seq_length])\n",
        "\n",
        "        x = torch.relu(self.conv1(x))  # Apply conv1 and pooling\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 and pooling\n",
        "        #x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 and pooling\n",
        "\n",
        "        x = x.view(-1, 16 * (x.shape[2]))  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Apply first fully connected layer\n",
        "        x = self.fc2(x)  # Output layer (no activation since we'll apply softmax in loss)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Loss function (cross-entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "O_gyZYu9XqYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "num_epochs = 3  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ufOIrAbaW1F",
        "outputId": "e93de07b-9973-41eb-b535-331cf2e3b713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.2979, Accuracy: 0.9041\n",
            "Epoch 2/3, Loss: 0.1936, Accuracy: 0.9378\n",
            "Epoch 3/3, Loss: 0.1731, Accuracy: 0.9447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_preds = (predicted == y_test).sum().item()\n",
        "    accuracy = correct_preds / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44agAuPBeDNd",
        "outputId": "fb075590-cbbb-45ef-a68d-d152490c3aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7133\n"
          ]
        }
      ]
    }
  ]
}