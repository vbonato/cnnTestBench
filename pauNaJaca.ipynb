{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18jg9Sa2IiZ2BMKbRW3NEQwxqxb9n3DH0",
      "authorship_tag": "ABX9TyOEaX+VFfUElijZoFRwmFzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbonato/cnnTestBench/blob/main/pauNaJaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBoqo2tH2d6g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v9NgXTUH3Z0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Dataset WISDM**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IVtz1oB_0MmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOAdb2nepkTb",
        "outputId": "2bd5da09-a1d5-48cc-e259-e949b774e3b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time windows\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "# Function to create time windows\n",
        "def create_time_windows(data, labels, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])  # Select window of data\n",
        "        y.append(labels[i + (window_size-1)])  # Label is from the last element of the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to save windowed data for your C++ program\n",
        "def save_windowed_data_to_dat(X, y, file_path):\n",
        "    \"\"\"Saves the windowed data to a .dat file.\"\"\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        for i in range(X.shape[0]):\n",
        "            feature_vector = X[i].flatten()\n",
        "            feature_str = \",\".join(map(str, feature_vector))\n",
        "            label = y[i]\n",
        "            f.write(f\"{{{{{feature_str}}},{label}}}\\n\")\n",
        "    print(f\"Successfully saved {X.shape[0]} samples to {file_path}\")\n",
        "\n",
        "# Updated data loading function\n",
        "def load_data(file_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Use regular expression to match the pattern {{feature_vector}, label}\n",
        "            match = re.match(r\"\\{\\{([0-9.,-]+)\\},\\s*(\\d+)\\}\", line.strip())\n",
        "\n",
        "            if match:\n",
        "                # Extract feature vector and label\n",
        "                feature_str = match.group(1)  # The feature string \"8.24,-2.11,3.87\"\n",
        "                label = int(match.group(2))  # The label \"4\"\n",
        "\n",
        "                # Convert the feature string to a list of floats\n",
        "                feature_vector = list(map(float, feature_str.split(',')))\n",
        "\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load train and test data\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/WISDM/HAR-Dataset/train.dat'  # Adjust path to your file\n",
        "test_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/WISDM/HAR-Dataset/test.dat'  # Adjust path to your file\n",
        "\n",
        "# Define the window size\n",
        "window_size = 10\n",
        "\n",
        "# Create time windows\n",
        "X_train, y_train = load_data(train_data_file)\n",
        "X_train, y_train = create_time_windows(X_train, y_train, window_size)\n",
        "\n",
        "X_test, y_test = load_data(test_data_file)\n",
        "X_test, y_test = create_time_windows(X_test, y_test, window_size)\n",
        "\n",
        "# --- 2. START: SAVE DATA FOR C++ TRAINER ---\n",
        "# Define where you want to save the new files\n",
        "output_path_train = 'train_windowed.dat' # You can change this path\n",
        "output_path_test = 'test_windowed.dat'   # You can change this path\n",
        "\n",
        "# Call the save function\n",
        "save_windowed_data_to_dat(X_train, y_train, output_path_train)\n",
        "save_windowed_data_to_dat(X_test, y_test, output_path_test)\n",
        "# --- END: SAVE DATA FOR C++ TRAINER ---\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)  # For classification (long type for labels)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yh6d-g0vBXZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b292640b-1785-4a33-e045-b072b0460a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 738432 samples to train_windowed.dat\n",
            "Successfully saved 316462 samples to test_windowed.dat\n",
            "First feature vector in X_train:\n",
            "[[-0.6946377  12.680544    0.50395286]\n",
            " [ 5.012288   11.264028    0.95342433]\n",
            " [ 4.903325   10.882658   -0.08172209]\n",
            " [-0.61291564 18.496431    3.0237172 ]\n",
            " [-1.1849703  12.108489    7.205164  ]\n",
            " [ 1.3756552  -2.4925237  -6.510526  ]\n",
            " [-0.61291564 10.56939     5.706926  ]\n",
            " [-0.50395286 13.947236    7.0553403 ]\n",
            " [-8.430995   11.413852    5.134871  ]\n",
            " [ 0.95342433  1.3756552   1.6480621 ]]\n",
            "First label in y_train:\n",
            "0\n",
            "First feature vector in X_train:\n",
            "tensor([[-0.6946, 12.6805,  0.5040],\n",
            "        [ 5.0123, 11.2640,  0.9534],\n",
            "        [ 4.9033, 10.8827, -0.0817],\n",
            "        [-0.6129, 18.4964,  3.0237],\n",
            "        [-1.1850, 12.1085,  7.2052],\n",
            "        [ 1.3757, -2.4925, -6.5105],\n",
            "        [-0.6129, 10.5694,  5.7069],\n",
            "        [-0.5040, 13.9472,  7.0553],\n",
            "        [-8.4310, 11.4139,  5.1349],\n",
            "        [ 0.9534,  1.3757,  1.6481]])\n",
            "First label in y_train:\n",
            "tensor(0)\n",
            "X_train shape: torch.Size([738432, 10, 3]), y_train shape: torch.Size([738432])\n",
            "X_test shape: torch.Size([316462, 10, 3]), y_test shape: torch.Size([316462])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "        #self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the input size for the fully connected layer based on the output size of conv1\n",
        "        self.fc1 = nn.Linear(16 * (X_train.shape[1] // 2), 64)  # Flattened size after pooling\n",
        "        self.fc2 = nn.Linear(64, 6)  # Assuming 6 classes for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Add channel dimension (change shape to [batch_size, channels, seq_length])\n",
        "\n",
        "        #x = torch.relu(self.conv1(x))  # Apply conv1 and pooling\n",
        "        #x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 and pooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 and pooling\n",
        "\n",
        "        x = x.view(-1, 16 * (x.shape[2]))  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Apply first fully connected layer\n",
        "        x = self.fc2(x)  # Output layer (no activation since we'll apply softmax in loss)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Loss function (cross-entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "g2wLGd2VBRmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 3  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "id": "tGh-L7AvBg3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3cf9d2-d7c9-43ea-ed22-05b0e14259a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4274, Accuracy: 0.8497\n",
            "Epoch 2/3, Loss: 0.3366, Accuracy: 0.8841\n",
            "Epoch 3/3, Loss: 0.3117, Accuracy: 0.8936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_preds = (predicted == y_test).sum().item()\n",
        "    accuracy = correct_preds / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIhh3n6iBlDB",
        "outputId": "db7c5010-2820-4ec7-aafe-caefc23acee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'simple_cnn_model.pth')\n",
        "\n",
        "# Load the model (if needed)\n",
        "model = SimpleCNN()\n",
        "model.load_state_dict(torch.load('simple_cnn_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b3hDlz-mBoIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Dataset PAMAP2**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aB498o73z9yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove columns heartrate and temp of the three IMUs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(train_data_file)\n",
        "\n",
        "# Remove columns\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df = df.drop(df.columns[11], axis=1)\n",
        "df = df.drop(df.columns[20], axis=1)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# Print a message to confirm that the file was saved\n",
        "print(f\"Updated CSV saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "ybJnxdyB7hhw",
        "outputId": "e95a154c-3454-4002-e5fa-b8647a6c0cd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated CSV saved to: /content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the activity names in the first column with their corresponding IDs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the modified CSV\n",
        "modified_train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv'\n",
        "\n",
        "# Load the modified CSV into a DataFrame without assuming a header\n",
        "df = pd.read_csv(modified_train_data_file, header=None)\n",
        "\n",
        "# Print the first few rows to check the content\n",
        "print(\"Original DataFrame first few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Create a dictionary to map activity names to activity IDs\n",
        "activity_map = {\n",
        "    'lying': 1,\n",
        "    'sitting': 2,\n",
        "    'standing': 3,\n",
        "    'walking': 4,\n",
        "    'running': 5,\n",
        "    'cycling': 6,\n",
        "    'nordic_walking': 7,\n",
        "    'watching_TV': 9,\n",
        "    'computer_work': 10,\n",
        "    'car_driving': 11,\n",
        "    'ascending_stairs': 12,\n",
        "    'descending_stairs': 13,\n",
        "    'vacuum_cleaning': 16,\n",
        "    'ironing': 17,\n",
        "    'folding_laundry': 18,\n",
        "    'house_cleaning': 19,\n",
        "    'playing_soccer': 20,\n",
        "    'rope_jumping': 24,\n",
        "    'other': 0\n",
        "}\n",
        "\n",
        "# Replace the activity names in the first column with their corresponding IDs\n",
        "df.iloc[:, 0] = df.iloc[:, 0].map(activity_map)\n",
        "\n",
        "# Print the first few rows after the transformation to ensure it's working\n",
        "print(\"\\nDataFrame after replacing activity names with IDs:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv'\n",
        "df.to_csv(output_file, index=False, header=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"Updated CSV with activity IDs saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "zuGjD4IYE8mR",
        "outputId": "3780ea18-488e-4b34-8363-2f9a4584cbb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame first few rows:\n",
            "      0    1        2        3        4         5         6         7   \\\n",
            "0  lying  1.0  2.21530  8.27915  5.58753 -0.004750  0.037579 -0.011145   \n",
            "1  lying  1.0  2.29196  7.67288  5.74467 -0.171710  0.025479 -0.009538   \n",
            "2  lying  1.0  2.29090  7.14240  5.82342 -0.238241  0.011214  0.000831   \n",
            "3  lying  1.0  2.21800  7.14365  5.89930 -0.192912  0.019053  0.013374   \n",
            "4  lying  1.0  2.30106  7.25857  6.09259 -0.069961 -0.018328  0.004582   \n",
            "\n",
            "        8        9   ...       19       20       21        22        23  \\\n",
            "0  8.93200 -67.9326  ...  44.2728  9.73855 -1.84761  0.095156  0.002908   \n",
            "1  9.58300 -67.9584  ...  43.5427  9.69762 -1.88438 -0.020804  0.020882   \n",
            "2  9.05516 -67.4017  ...  44.0259  9.69633 -1.92203 -0.059173 -0.035392   \n",
            "3  9.92698 -67.4387  ...  43.6570  9.66370 -1.84714  0.094385 -0.032514   \n",
            "4  9.15626 -67.1825  ...  42.9228  9.77578 -1.88582  0.095775  0.001351   \n",
            "\n",
            "         24        25       26       27       28  \n",
            "0 -0.027714  0.001752 -61.1081 -36.8636 -58.3696  \n",
            "1  0.000945  0.006007 -60.8916 -36.3197 -58.3656  \n",
            "2 -0.052422 -0.004882 -60.3407 -35.7842 -58.6119  \n",
            "3 -0.018844  0.026950 -60.7646 -37.1028 -57.8799  \n",
            "4 -0.048878 -0.006328 -60.2040 -37.1225 -57.8847  \n",
            "\n",
            "[5 rows x 29 columns]\n",
            "\n",
            "DataFrame after replacing activity names with IDs:\n",
            "  0    1        2        3        4         5         6         7        8   \\\n",
            "0  1  1.0  2.21530  8.27915  5.58753 -0.004750  0.037579 -0.011145  8.93200   \n",
            "1  1  1.0  2.29196  7.67288  5.74467 -0.171710  0.025479 -0.009538  9.58300   \n",
            "2  1  1.0  2.29090  7.14240  5.82342 -0.238241  0.011214  0.000831  9.05516   \n",
            "3  1  1.0  2.21800  7.14365  5.89930 -0.192912  0.019053  0.013374  9.92698   \n",
            "4  1  1.0  2.30106  7.25857  6.09259 -0.069961 -0.018328  0.004582  9.15626   \n",
            "\n",
            "        9   ...       19       20       21        22        23        24  \\\n",
            "0 -67.9326  ...  44.2728  9.73855 -1.84761  0.095156  0.002908 -0.027714   \n",
            "1 -67.9584  ...  43.5427  9.69762 -1.88438 -0.020804  0.020882  0.000945   \n",
            "2 -67.4017  ...  44.0259  9.69633 -1.92203 -0.059173 -0.035392 -0.052422   \n",
            "3 -67.4387  ...  43.6570  9.66370 -1.84714  0.094385 -0.032514 -0.018844   \n",
            "4 -67.1825  ...  42.9228  9.77578 -1.88582  0.095775  0.001351 -0.048878   \n",
            "\n",
            "         25       26       27       28  \n",
            "0  0.001752 -61.1081 -36.8636 -58.3696  \n",
            "1  0.006007 -60.8916 -36.3197 -58.3656  \n",
            "2 -0.004882 -60.3407 -35.7842 -58.6119  \n",
            "3  0.026950 -60.7646 -37.1028 -57.8799  \n",
            "4 -0.006328 -60.2040 -37.1225 -57.8847  \n",
            "\n",
            "[5 rows x 29 columns]\n",
            "Updated CSV with activity IDs saved to: /content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split CSV in train (excluding user 5) and test (only user 5) csv files\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the CSV file containing activity IDs\n",
        "train_data_file_with_ids = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame without assuming a header\n",
        "df = pd.read_csv(train_data_file_with_ids, header=None)\n",
        "\n",
        "# Print the first few rows to check the content\n",
        "print(\"Original DataFrame first few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Filter rows where the user ID is 5 and save them as test_pamap2.csv\n",
        "test_df = df[df.iloc[:, 1] == 5]\n",
        "test_df.to_csv('/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.csv', index=False, header=False)\n",
        "\n",
        "# Filter rows where the user ID is not 5 (excluding) and save them as train_pamap2.csv\n",
        "train_df = df[df.iloc[:, 1] != 5]\n",
        "train_df.to_csv('/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.csv', index=False, header=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Data has been split into train_pamap2.csv (excluding user ID 5) and test_pamap2.csv (user ID 5 only).\")\n"
      ],
      "metadata": {
        "id": "ColCz8mlHdJS",
        "outputId": "e15a48f4-0864-44ab-ecee-5f4ee5dd24be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame first few rows:\n",
            "   0    1        2        3        4         5         6         7        8   \\\n",
            "0   1  1.0  2.21530  8.27915  5.58753 -0.004750  0.037579 -0.011145  8.93200   \n",
            "1   1  1.0  2.29196  7.67288  5.74467 -0.171710  0.025479 -0.009538  9.58300   \n",
            "2   1  1.0  2.29090  7.14240  5.82342 -0.238241  0.011214  0.000831  9.05516   \n",
            "3   1  1.0  2.21800  7.14365  5.89930 -0.192912  0.019053  0.013374  9.92698   \n",
            "4   1  1.0  2.30106  7.25857  6.09259 -0.069961 -0.018328  0.004582  9.15626   \n",
            "\n",
            "        9   ...       19       20       21        22        23        24  \\\n",
            "0 -67.9326  ...  44.2728  9.73855 -1.84761  0.095156  0.002908 -0.027714   \n",
            "1 -67.9584  ...  43.5427  9.69762 -1.88438 -0.020804  0.020882  0.000945   \n",
            "2 -67.4017  ...  44.0259  9.69633 -1.92203 -0.059173 -0.035392 -0.052422   \n",
            "3 -67.4387  ...  43.6570  9.66370 -1.84714  0.094385 -0.032514 -0.018844   \n",
            "4 -67.1825  ...  42.9228  9.77578 -1.88582  0.095775  0.001351 -0.048878   \n",
            "\n",
            "         25       26       27       28  \n",
            "0  0.001752 -61.1081 -36.8636 -58.3696  \n",
            "1  0.006007 -60.8916 -36.3197 -58.3656  \n",
            "2 -0.004882 -60.3407 -35.7842 -58.6119  \n",
            "3  0.026950 -60.7646 -37.1028 -57.8799  \n",
            "4 -0.006328 -60.2040 -37.1225 -57.8847  \n",
            "\n",
            "[5 rows x 29 columns]\n",
            "Data has been split into train_pamap2.csv (excluding user ID 5) and test_pamap2.csv (user ID 5 only).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert .csv to .dat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "train_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.csv'\n",
        "test_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.csv'\n",
        "\n",
        "# Function to convert rows into the required format and save as .dat\n",
        "def convert_to_dat(input_file, output_file):\n",
        "    # Load the CSV file without a header\n",
        "    df = pd.read_csv(input_file, header=None)\n",
        "\n",
        "    # Open the output .dat file to write\n",
        "    with open(output_file, 'w') as file:\n",
        "        for _, row in df.iterrows():\n",
        "            # Convert row into the required format: {{column 2, column 3, column 4, ...}, column 0}\n",
        "            row_data = \"{{\" + \",\".join(map(str, row[2:])) + \"},\" + str(int(row[0])) + \"}\\n\"\n",
        "            file.write(row_data)\n",
        "\n",
        "# Convert train_pamap2.csv to train_pamap2.dat\n",
        "convert_to_dat(train_file, '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.dat')\n",
        "\n",
        "# Convert test_pamap2.csv to test_pamap2.dat\n",
        "convert_to_dat(test_file, '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.dat')\n",
        "\n",
        "print(\"Conversion to .dat files is complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cncd9etsQcGQ",
        "outputId": "8eb2eae2-5f40-44ab-fe02-d35379e1699a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion to .dat files is complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time windows\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "# Function to create time windows\n",
        "def create_time_windows(data, labels, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])  # Select window of data\n",
        "        y.append(labels[i + (window_size-1)])  # Label is from the last element of the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to save windowed data for your C++ program\n",
        "def save_windowed_data_to_dat(X, y, file_path):\n",
        "    \"\"\"Saves the windowed data to a .dat file.\"\"\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        for i in range(X.shape[0]):\n",
        "            feature_vector = X[i].flatten()\n",
        "            feature_str = \",\".join(map(str, feature_vector))\n",
        "            label = y[i]\n",
        "            f.write(f\"{{{{{feature_str}}},{label}}}\\n\")\n",
        "    print(f\"Successfully saved {X.shape[0]} samples to {file_path}\")\n",
        "\n",
        "# Updated data loading function\n",
        "def load_data(file_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Use regular expression to match the pattern {{feature_vector}, label}\n",
        "            match = re.match(r\"\\{\\{([0-9.,-]+)\\},\\s*(\\d+)\\}\", line.strip())\n",
        "\n",
        "            if match:\n",
        "                # Extract feature vector and label\n",
        "                feature_str = match.group(1)  # The feature string \"8.24,-2.11,3.87\"\n",
        "                label = int(match.group(2))  # The label \"4\"\n",
        "\n",
        "                # Convert the feature string to a list of floats\n",
        "                feature_vector = list(map(float, feature_str.split(',')))\n",
        "\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load train and test data\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.dat'  # Adjust path to your file\n",
        "test_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.dat'  # Adjust path to your file\n",
        "\n",
        "print(\"File loaded:\")\n",
        "\n",
        "# Define the window size\n",
        "window_size = 10\n",
        "\n",
        "# Create time windows\n",
        "X_train, y_train = load_data(train_data_file)\n",
        "X_train, y_train = create_time_windows(X_train, y_train, window_size)\n",
        "\n",
        "X_test, y_test = load_data(test_data_file)\n",
        "X_test, y_test = create_time_windows(X_test, y_test, window_size)\n",
        "print(\"Time window created:\")\n",
        "\n",
        "# --- 2. START: SAVE DATA FOR C++ TRAINER ---\n",
        "# Define where you want to save the new files\n",
        "output_path_train = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_windowed.dat' # You can change this path\n",
        "output_path_test = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_windowed.dat'   # You can change this path\n",
        "\n",
        "# Call the save function\n",
        "save_windowed_data_to_dat(X_train, y_train, output_path_train)\n",
        "save_windowed_data_to_dat(X_test, y_test, output_path_test)\n",
        "# --- END: SAVE DATA FOR C++ TRAINER ---\n",
        "\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"Second feature vector in X_train:\")\n",
        "print(X_train[1])  # First row (first feature vector)\n",
        "\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "print(\"Second label in y_train:\")\n",
        "print(y_train[1])  # First label\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)  # For classification (long type for labels)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# # Print the first feature vector and label\n",
        "# print(\"First feature vector in X_train:\")\n",
        "# print(X_train[0])  # First row (first feature vector)\n",
        "# print(\"First label in y_train:\")\n",
        "# print(y_train[0])  # First label\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "2cslxq8BXNTk",
        "outputId": "1f9b0d24-59f3-457e-8801-72908b51462b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded:\n",
            "Time window created:\n",
            "Successfully saved 1655784 samples to /content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_windowed.dat\n",
            "Successfully saved 270616 samples to /content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_windowed.dat\n",
            "First feature vector in X_train:\n",
            "[[ 2.21530e+00  8.27915e+00  5.58753e+00 -4.75004e-03  3.75788e-02\n",
            "  -1.11450e-02  8.93200e+00 -6.79326e+01 -1.99755e+01  1.24482e-01\n",
            "   9.65003e+00 -1.65181e+00  3.66684e-02  1.65592e-02 -5.27913e-02\n",
            "   5.67566e-01 -5.07269e+01  4.42728e+01  9.73855e+00 -1.84761e+00\n",
            "   9.51561e-02  2.90826e-03 -2.77138e-02  1.75228e-03 -6.11081e+01\n",
            "  -3.68636e+01 -5.83696e+01]\n",
            " [ 2.29196e+00  7.67288e+00  5.74467e+00 -1.71710e-01  2.54788e-02\n",
            "  -9.53821e-03  9.58300e+00 -6.79584e+01 -2.09091e+01  2.00711e-01\n",
            "   9.64980e+00 -1.65043e+00  1.93427e-02 -2.43039e-02 -5.98426e-02\n",
            "   9.04990e-01 -5.05080e+01  4.35427e+01  9.69762e+00 -1.88438e+00\n",
            "  -2.08042e-02  2.08820e-02  9.44724e-04  6.00704e-03 -6.08916e+01\n",
            "  -3.63197e+01 -5.83656e+01]\n",
            " [ 2.29090e+00  7.14240e+00  5.82342e+00 -2.38241e-01  1.12136e-02\n",
            "   8.30722e-04  9.05516e+00 -6.74017e+01 -1.95083e+01  2.70277e-01\n",
            "   9.72331e+00 -1.88174e+00 -1.42782e-03  3.84664e-02 -4.64641e-02\n",
            "   4.55480e-01 -5.07209e+01  4.40259e+01  9.69633e+00 -1.92203e+00\n",
            "  -5.91734e-02 -3.53922e-02 -5.24217e-02 -4.88214e-03 -6.03407e+01\n",
            "  -3.57842e+01 -5.86119e+01]\n",
            " [ 2.21800e+00  7.14365e+00  5.89930e+00 -1.92912e-01  1.90534e-02\n",
            "   1.33742e-02  9.92698e+00 -6.74387e+01 -2.05602e+01  2.36737e-01\n",
            "   9.72447e+00 -1.72746e+00  1.72770e-02 -4.85468e-02 -7.49461e-02\n",
            "   3.24284e-01 -5.01544e+01  4.36570e+01  9.66370e+00 -1.84714e+00\n",
            "   9.43855e-02 -3.25136e-02 -1.88436e-02  2.69499e-02 -6.07646e+01\n",
            "  -3.71028e+01 -5.78799e+01]\n",
            " [ 2.30106e+00  7.25857e+00  6.09259e+00 -6.99614e-02 -1.83275e-02\n",
            "   4.58168e-03  9.15626e+00 -6.71825e+01 -2.00857e+01  3.52225e-01\n",
            "   9.72437e+00 -1.68665e+00  2.74589e-04 -1.33517e-02 -3.93151e-02\n",
            "   4.62317e-01 -5.07110e+01  4.29228e+01  9.77578e+00 -1.88582e+00\n",
            "   9.57749e-02  1.35137e-03 -4.88780e-02 -6.32768e-03 -6.02040e+01\n",
            "  -3.71225e+01 -5.78847e+01]\n",
            " [ 2.07165e+00  7.25965e+00  6.01218e+00  6.38951e-02  7.17462e-03\n",
            "   2.47007e-02  8.60505e+00 -6.67184e+01 -1.98429e+01  2.71221e-01\n",
            "   9.64826e+00 -1.84275e+00 -2.48122e-03 -1.81305e-02 -4.54859e-02\n",
            "   1.01082e+00 -5.04054e+01  4.41573e+01  9.65780e+00 -1.92147e+00\n",
            "  -9.81615e-02  3.79324e-03 -2.69057e-02  4.12540e-03 -6.13257e+01\n",
            "  -3.69744e+01 -5.77501e+01]\n",
            " [ 2.41148e+00  7.59780e+00  5.93915e+00  1.90837e-01  3.11615e-03\n",
            "   3.87616e-02  9.16003e+00 -6.68578e+01 -1.99665e+01  2.74553e-01\n",
            "   9.61139e+00 -1.72641e+00  2.46086e-02 -2.24515e-02 -3.37392e-02\n",
            "   1.26170e+00 -5.09669e+01  4.35475e+01  9.65839e+00 -1.80756e+00\n",
            "  -9.88623e-02  3.68139e-02 -3.22766e-02 -6.86640e-03 -6.15520e+01\n",
            "  -3.69632e+01 -5.79957e+01]\n",
            " [ 2.32815e+00  7.63431e+00  5.70686e+00  2.00328e-01 -9.26579e-03\n",
            "   6.85673e-02  9.47898e+00 -6.73025e+01 -2.06693e+01  2.38925e-01\n",
            "   9.68733e+00 -1.64986e+00  1.24771e-02 -3.88337e-04 -3.17048e-02\n",
            "   3.55493e-01 -5.11679e+01  4.43915e+01  9.65690e+00 -1.88318e+00\n",
            "  -1.36998e-01 -1.03518e-02 -1.66206e-02  6.54785e-03 -6.15738e+01\n",
            "  -3.61724e+01 -5.93487e+01]\n",
            " [ 2.25096e+00  7.78598e+00  5.62821e+00  2.04098e-01 -6.82564e-02\n",
            "   5.00003e-02  9.15218e+00 -6.70705e+01 -2.03159e+01  3.49738e-01\n",
            "   9.64854e+00 -1.76389e+00  1.78925e-02 -1.82868e-02 -3.70889e-02\n",
            "   1.25653e-01 -5.09288e+01  4.35303e+01  9.77727e+00 -1.81020e+00\n",
            "   1.33911e-01  3.93458e-02  2.03933e-02 -1.18800e-02 -6.17741e+01\n",
            "  -3.71744e+01 -5.81199e+01]\n",
            " [ 2.14107e+00  7.52262e+00  5.78141e+00  1.71291e-01 -5.54110e-02\n",
            "   2.15762e-02  9.49746e+00 -6.67700e+01 -1.98537e+01  1.20851e-01\n",
            "   9.57395e+00 -1.76780e+00  3.23241e-03 -2.78503e-02 -1.53783e-02\n",
            "   9.20127e-01 -5.09566e+01  4.36649e+01  9.69492e+00 -1.76951e+00\n",
            "  -1.37313e-01  2.98741e-02 -1.07634e-02  5.13317e-03 -6.07680e+01\n",
            "  -3.74206e+01 -5.88735e+01]]\n",
            "Second feature vector in X_train:\n",
            "[[ 2.29196e+00  7.67288e+00  5.74467e+00 -1.71710e-01  2.54788e-02\n",
            "  -9.53821e-03  9.58300e+00 -6.79584e+01 -2.09091e+01  2.00711e-01\n",
            "   9.64980e+00 -1.65043e+00  1.93427e-02 -2.43039e-02 -5.98426e-02\n",
            "   9.04990e-01 -5.05080e+01  4.35427e+01  9.69762e+00 -1.88438e+00\n",
            "  -2.08042e-02  2.08820e-02  9.44724e-04  6.00704e-03 -6.08916e+01\n",
            "  -3.63197e+01 -5.83656e+01]\n",
            " [ 2.29090e+00  7.14240e+00  5.82342e+00 -2.38241e-01  1.12136e-02\n",
            "   8.30722e-04  9.05516e+00 -6.74017e+01 -1.95083e+01  2.70277e-01\n",
            "   9.72331e+00 -1.88174e+00 -1.42782e-03  3.84664e-02 -4.64641e-02\n",
            "   4.55480e-01 -5.07209e+01  4.40259e+01  9.69633e+00 -1.92203e+00\n",
            "  -5.91734e-02 -3.53922e-02 -5.24217e-02 -4.88214e-03 -6.03407e+01\n",
            "  -3.57842e+01 -5.86119e+01]\n",
            " [ 2.21800e+00  7.14365e+00  5.89930e+00 -1.92912e-01  1.90534e-02\n",
            "   1.33742e-02  9.92698e+00 -6.74387e+01 -2.05602e+01  2.36737e-01\n",
            "   9.72447e+00 -1.72746e+00  1.72770e-02 -4.85468e-02 -7.49461e-02\n",
            "   3.24284e-01 -5.01544e+01  4.36570e+01  9.66370e+00 -1.84714e+00\n",
            "   9.43855e-02 -3.25136e-02 -1.88436e-02  2.69499e-02 -6.07646e+01\n",
            "  -3.71028e+01 -5.78799e+01]\n",
            " [ 2.30106e+00  7.25857e+00  6.09259e+00 -6.99614e-02 -1.83275e-02\n",
            "   4.58168e-03  9.15626e+00 -6.71825e+01 -2.00857e+01  3.52225e-01\n",
            "   9.72437e+00 -1.68665e+00  2.74589e-04 -1.33517e-02 -3.93151e-02\n",
            "   4.62317e-01 -5.07110e+01  4.29228e+01  9.77578e+00 -1.88582e+00\n",
            "   9.57749e-02  1.35137e-03 -4.88780e-02 -6.32768e-03 -6.02040e+01\n",
            "  -3.71225e+01 -5.78847e+01]\n",
            " [ 2.07165e+00  7.25965e+00  6.01218e+00  6.38951e-02  7.17462e-03\n",
            "   2.47007e-02  8.60505e+00 -6.67184e+01 -1.98429e+01  2.71221e-01\n",
            "   9.64826e+00 -1.84275e+00 -2.48122e-03 -1.81305e-02 -4.54859e-02\n",
            "   1.01082e+00 -5.04054e+01  4.41573e+01  9.65780e+00 -1.92147e+00\n",
            "  -9.81615e-02  3.79324e-03 -2.69057e-02  4.12540e-03 -6.13257e+01\n",
            "  -3.69744e+01 -5.77501e+01]\n",
            " [ 2.41148e+00  7.59780e+00  5.93915e+00  1.90837e-01  3.11615e-03\n",
            "   3.87616e-02  9.16003e+00 -6.68578e+01 -1.99665e+01  2.74553e-01\n",
            "   9.61139e+00 -1.72641e+00  2.46086e-02 -2.24515e-02 -3.37392e-02\n",
            "   1.26170e+00 -5.09669e+01  4.35475e+01  9.65839e+00 -1.80756e+00\n",
            "  -9.88623e-02  3.68139e-02 -3.22766e-02 -6.86640e-03 -6.15520e+01\n",
            "  -3.69632e+01 -5.79957e+01]\n",
            " [ 2.32815e+00  7.63431e+00  5.70686e+00  2.00328e-01 -9.26579e-03\n",
            "   6.85673e-02  9.47898e+00 -6.73025e+01 -2.06693e+01  2.38925e-01\n",
            "   9.68733e+00 -1.64986e+00  1.24771e-02 -3.88337e-04 -3.17048e-02\n",
            "   3.55493e-01 -5.11679e+01  4.43915e+01  9.65690e+00 -1.88318e+00\n",
            "  -1.36998e-01 -1.03518e-02 -1.66206e-02  6.54785e-03 -6.15738e+01\n",
            "  -3.61724e+01 -5.93487e+01]\n",
            " [ 2.25096e+00  7.78598e+00  5.62821e+00  2.04098e-01 -6.82564e-02\n",
            "   5.00003e-02  9.15218e+00 -6.70705e+01 -2.03159e+01  3.49738e-01\n",
            "   9.64854e+00 -1.76389e+00  1.78925e-02 -1.82868e-02 -3.70889e-02\n",
            "   1.25653e-01 -5.09288e+01  4.35303e+01  9.77727e+00 -1.81020e+00\n",
            "   1.33911e-01  3.93458e-02  2.03933e-02 -1.18800e-02 -6.17741e+01\n",
            "  -3.71744e+01 -5.81199e+01]\n",
            " [ 2.14107e+00  7.52262e+00  5.78141e+00  1.71291e-01 -5.54110e-02\n",
            "   2.15762e-02  9.49746e+00 -6.67700e+01 -1.98537e+01  1.20851e-01\n",
            "   9.57395e+00 -1.76780e+00  3.23241e-03 -2.78503e-02 -1.53783e-02\n",
            "   9.20127e-01 -5.09566e+01  4.36649e+01  9.69492e+00 -1.76951e+00\n",
            "  -1.37313e-01  2.98741e-02 -1.07634e-02  5.13317e-03 -6.07680e+01\n",
            "  -3.74206e+01 -5.88735e+01]\n",
            " [ 2.36727e+00  7.63436e+00  5.74593e+00  9.97821e-02 -3.85650e-02\n",
            "   1.11589e-02  8.83009e+00 -6.78239e+01 -1.95102e+01  1.99667e-01\n",
            "   9.68719e+00 -1.68929e+00  3.99616e-02  1.74107e-02 -1.92156e-02\n",
            "   1.38419e+00 -5.13107e+01  4.40392e+01  9.72955e+00 -1.88301e+00\n",
            "  -2.13433e-01  1.86298e-02  1.00099e-02  1.00248e-02 -6.07654e+01\n",
            "  -3.75326e+01 -5.87509e+01]]\n",
            "First label in y_train:\n",
            "1\n",
            "Second label in y_train:\n",
            "1\n",
            "X_train shape: torch.Size([1655784, 10, 27]), y_train shape: torch.Size([1655784])\n",
            "X_test shape: torch.Size([270616, 10, 27]), y_test shape: torch.Size([270616])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=27, out_channels=27, kernel_size=3, padding=1)  # input channels = number of features\n",
        "        self.conv2 = nn.Conv1d(in_channels=27, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the input size for the fully connected layer based on the output size of conv1\n",
        "        self.fc1 = nn.Linear(16 * (X_train.shape[1] // 2), 64)  # Flattened size after pooling\n",
        "        self.fc2 = nn.Linear(64, 25)  # Assuming 25 classes for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Add channel dimension (change shape to [batch_size, channels, seq_length])\n",
        "\n",
        "        x = torch.relu(self.conv1(x))  # Apply conv1 and pooling\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 and pooling\n",
        "        #x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 and pooling\n",
        "\n",
        "        x = x.view(-1, 16 * (x.shape[2]))  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Apply first fully connected layer\n",
        "        x = self.fc2(x)  # Output layer (no activation since we'll apply softmax in loss)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Loss function (cross-entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "O_gyZYu9XqYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "num_epochs = 3  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ufOIrAbaW1F",
        "outputId": "e93de07b-9973-41eb-b535-331cf2e3b713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.2979, Accuracy: 0.9041\n",
            "Epoch 2/3, Loss: 0.1936, Accuracy: 0.9378\n",
            "Epoch 3/3, Loss: 0.1731, Accuracy: 0.9447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_preds = (predicted == y_test).sum().item()\n",
        "    accuracy = correct_preds / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44agAuPBeDNd",
        "outputId": "fb075590-cbbb-45ef-a68d-d152490c3aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7133\n"
          ]
        }
      ]
    }
  ]
}