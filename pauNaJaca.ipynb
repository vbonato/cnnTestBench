{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/vbonato/cnnTestBench/blob/main/pauNaJaca.ipynb",
      "authorship_tag": "ABX9TyNhOpm5NkxKVsB6G9n2/sUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbonato/cnnTestBench/blob/main/pauNaJaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBoqo2tH2d6g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v9NgXTUH3Z0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_vdVclBcP1d4",
        "outputId": "3333dd9d-1d83-4c27-fa6b-bcc0476a92c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Dataset WISDM**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IVtz1oB_0MmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time windows\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "# Function to create time windows\n",
        "def create_time_windows(data, labels, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])  # Select window of data\n",
        "        y.append(labels[i + (window_size-1)])  # Label is from the last element of the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Updated data loading function\n",
        "def load_data(file_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Use regular expression to match the pattern {{feature_vector}, label}\n",
        "            match = re.match(r\"\\{\\{([0-9.,-]+)\\},\\s*(\\d+)\\}\", line.strip())\n",
        "\n",
        "            if match:\n",
        "                # Extract feature vector and label\n",
        "                feature_str = match.group(1)  # The feature string \"8.24,-2.11,3.87\"\n",
        "                label = int(match.group(2))  # The label \"4\"\n",
        "\n",
        "                # Convert the feature string to a list of floats\n",
        "                feature_vector = list(map(float, feature_str.split(',')))\n",
        "\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load train and test data\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/WISDM/HAR-Dataset/train.dat'  # Adjust path to your file\n",
        "test_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/WISDM/HAR-Dataset/test.dat'  # Adjust path to your file\n",
        "\n",
        "# Define the window size\n",
        "window_size = 100\n",
        "\n",
        "# Create time windows\n",
        "X_train, y_train = load_data(train_data_file)\n",
        "X_train, y_train = create_time_windows(X_train, y_train, window_size)\n",
        "\n",
        "X_test, y_test = load_data(test_data_file)\n",
        "X_test, y_test = create_time_windows(X_test, y_test, window_size)\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)  # For classification (long type for labels)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yh6d-g0vBXZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model and data normalization according to the train data set\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Assume 'train_data_file' is your original training data\n",
        "        X_train_temp, y_train_temp = load_data(train_data_file)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_train_temp)\n",
        "\n",
        "        # Just to print\n",
        "        original_means = scaler.mean_\n",
        "        original_stds = scaler.scale_\n",
        "        print(f\"MEANS = {original_means.tolist()}\")\n",
        "        print(f\"STDS = {original_stds.tolist()}\")\n",
        "\n",
        "        # Convert the numpy arrays to torch tensors\n",
        "        means_tensor = torch.tensor(scaler.mean_, dtype=torch.float32)\n",
        "        stds_tensor = torch.tensor(scaler.scale_, dtype=torch.float32)\n",
        "\n",
        "        # Register them as non-trainable buffers\n",
        "        # They are not considered model parameters to be trained.\n",
        "        self.register_buffer('means', means_tensor)\n",
        "        self.register_buffer('stds', stds_tensor)\n",
        "        # ------------------------------------\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "        #self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the input size for the fully connected layer based on the output size of conv1\n",
        "        self.fc1 = nn.Linear(16 * (X_train.shape[1] // 2), 64)  # Flattened size after pooling\n",
        "        self.fc2 = nn.Linear(64, 6)  # Assuming 6 classes for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x - self.means) / self.stds\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Add channel dimension (change shape to [batch_size, channels, seq_length])\n",
        "\n",
        "        #x = torch.relu(self.conv1(x))  # Apply conv1 and pooling\n",
        "        #x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 and pooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 and pooling\n",
        "\n",
        "        x = x.view(-1, 16 * (x.shape[2]))  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Apply first fully connected layer\n",
        "        x = self.fc2(x)  # Output layer (no activation since we'll apply softmax in loss)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Loss function (cross-entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "g2wLGd2VBRmq",
        "outputId": "cd64d0d8-7c90-4c40-d950-3b83dfc2ba11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEANS = [1.0182769835326968, 7.7139884937195875, 0.3890242449048468]\n",
            "STDS = [6.662064228097328, 6.77728797625153, 4.938553052818819]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 1  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "id": "tGh-L7AvBg3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee5c0c1-c18e-44c6-aef2-001652be34dc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Loss: 0.1807, Accuracy: 0.9392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_preds = (predicted == y_test).sum().item()\n",
        "    accuracy = correct_preds / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIhh3n6iBlDB",
        "outputId": "5f49e7e0-7dbd-4bdb-9448-242f79fe64ae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'simple_cnn_model.pth')\n",
        "\n",
        "# Load the model (if needed)\n",
        "model = SimpleCNN()\n",
        "model.load_state_dict(torch.load('simple_cnn_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b3hDlz-mBoIC",
        "outputId": "46f1090a-8f5c-4386-f512-d6cd52e28d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEANS = [1.0182769835326968, 7.7139884937195875, 0.3890242449048468]\n",
            "STDS = [6.662064228097328, 6.77728797625153, 4.938553052818819]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv1d(3, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=800, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Apply pruning and export whole model ---\n",
        "\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- Assume your SimpleCNN and data (X_train, y_train, X_test, y_test, window_size) are already defined --- #\n",
        "\n",
        "# Initialize the model and load trained weights\n",
        "model = SimpleCNN()\n",
        "model.load_state_dict(torch.load('simple_cnn_model.pth'))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# --- Optional: Evaluate original model accuracy ---\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    print(f\"Original Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# --- Apply pruning ---\n",
        "prune_amount = 0.3  # prune 30% of the weights\n",
        "\n",
        "# Layer-wise pruning\n",
        "prune.l1_unstructured(model.conv1, name='weight', amount=prune_amount)\n",
        "prune.l1_unstructured(model.fc1, name='weight', amount=prune_amount)\n",
        "prune.l1_unstructured(model.fc2, name='weight', amount=prune_amount)\n",
        "\n",
        "# --- Optional: Evaluate pruned model (with mask) ---\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    print(f\"Test Accuracy after pruning (with mask): {accuracy:.4f}\")\n",
        "\n",
        "# --- Make pruning permanent (remove mask) ---\n",
        "prune.remove(model.conv1, 'weight')\n",
        "prune.remove(model.fc1, 'weight')\n",
        "prune.remove(model.fc2, 'weight')\n",
        "\n",
        "# --- Evaluate pruned model (permanent pruning) ---\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    print(f\"Test Accuracy after permanent pruning: {accuracy:.4f}\")\n",
        "\n",
        "# --- Save pruned model weights ---\n",
        "torch.save(model.state_dict(), 'simple_cnn_pruned.pth')\n",
        "print(\"✅ Pruned model saved as simple_cnn_pruned.pth\")\n",
        "\n",
        "# --- Export the pruned model to ONNX for Netron ---\n",
        "dummy_input = torch.randn(1, window_size, 3)  # batch_size, seq_len, features\n",
        "onnx_file = \"simple_cnn_pruned.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,                # pruned model\n",
        "    dummy_input,          # dummy input\n",
        "    onnx_file,            # output file\n",
        "    export_params=True,   # store trained weights\n",
        "    opset_version=12,\n",
        "    input_names=['input'],\n",
        "    output_names=['output']\n",
        ")\n",
        "\n",
        "print(f\"✅ Pruned model exported to ONNX: {onnx_file}\")\n"
      ],
      "metadata": {
        "id": "lLrrKFFpIbOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx onnxruntime"
      ],
      "metadata": {
        "id": "ngIMSLXb5wal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBns2eu6wDcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Exports the model structure and weights into a single .onnx file ---\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load the saved weights from your training session\n",
        "model.load_state_dict(torch.load('simple_cnn_model.pth'))\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "# --- Export to ONNX ---\n",
        "# Create a dummy input tensor with the correct shape: [batch_size, seq_length, features]\n",
        "# This shape must match your model's input exactly.\n",
        "dummy_input = torch.randn(1, window_size, 3)\n",
        "\n",
        "# Define the output file name\n",
        "onnx_file = \"simple_cnn_wisdm.onnx\"\n",
        "\n",
        "print(f\"Exporting model to {onnx_file}...\")\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(model,               # The model to export\n",
        "                  dummy_input,         # A sample input\n",
        "                  onnx_file,      # Where to save the model\n",
        "                  export_params=True,  # Store the trained weights\n",
        "                  opset_version=12,    # The ONNX version to use\n",
        "                  input_names=['input'], # The name for the input tensor\n",
        "                  output_names=['output']) # The name for the output tensor\n",
        "\n",
        "print(\"Model has been successfully converted to ONNX format! ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osArRyfW69zk",
        "outputId": "e2434e9d-64c3-4a0a-8226-ff54127fbeec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting model to simple_cnn_wisdm.onnx...\n",
            "Model has been successfully converted to ONNX format! ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3304934217.py:20: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(model,               # The model to export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Convert from onnx to tf --- #\n",
        "\n",
        "!pip install onnx2tf\n",
        "!pip install onnx_graphsurgeon\n",
        "!pip install ai-edge-litert\n",
        "!pip install sng4onnx\n",
        "\n",
        "\n",
        "!onnx2tf -i simple_cnn_wisdm.onnx -o my_tf_model\n",
        "#!onnx2tf -i simple_cnn_pruned.onnx -o my_tf_model\n",
        "\n"
      ],
      "metadata": {
        "id": "jEeaGy2g3R6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Quantize the tf model using TFLM and export as .h --- #\n",
        "# --- TFLM (TensorFlow Lite for Microcontrollers) --- #\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np # Make sure numpy is imported\n",
        "\n",
        "# This function is your calibration step\n",
        "def representative_data_gen():\n",
        "    \"\"\"\n",
        "    Feeds sample data from X_train to the converter.\n",
        "    \"\"\"\n",
        "    # Convert X_train tensor to a NumPy array if it isn't already\n",
        "    # We only need a subset, e.g., the first 100 samples\n",
        "    x_train_np = X_train.numpy() if isinstance(X_train, torch.Tensor) else X_train\n",
        "    num_calibration_samples = min(100, x_train_np.shape[0]) # Use up to 100 samples\n",
        "\n",
        "    print(f\"Providing {num_calibration_samples} samples from X_train for calibration...\")\n",
        "\n",
        "    for i in range(num_calibration_samples):\n",
        "        # 1. Get one window from X_train. Shape: [window_size, features] (e.g., [100, 3])\n",
        "        sample = x_train_np[i]\n",
        "\n",
        "        # 2. Transpose to match model input: [features, window_size] (e.g., [3, 100])\n",
        "        #    This is CRUCIAL because Conv1D expects channels (features) first.\n",
        "        sample_transposed = np.transpose(sample, (1, 0))\n",
        "\n",
        "        # 3. Add the batch dimension: [1, features, window_size] (e.g., [1, 3, 100])\n",
        "        sample_batch = np.expand_dims(sample_transposed, axis=0).astype(np.float32)\n",
        "\n",
        "        #print(sample_batch)\n",
        "\n",
        "        # 4. Yield the data in the required list format\n",
        "        yield [sample_batch]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('my_tf_model')\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "\n",
        "with open('simple_cnn_quantized.tflite', 'wb') as f:\n",
        "#with open('simple_cnn_prunedANDquantized.tflite', 'wb') as f:\n",
        "    f.write(tflite_model_quant)\n",
        "\n",
        "print(\"✅ Success! Your quantized model has been saved as simple_cnn_xxx.tflite\")\n",
        "\n",
        "\n",
        "!xxd -i model_for_microbit.tflite > model_data.h"
      ],
      "metadata": {
        "id": "ZVI7LWZjxLpq",
        "outputId": "9369e806-ea23-4afe-86ca-36b83d93be57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Success! Your quantized model has been saved as simple_cnn_xxx.tflite\n",
            "xxd: model_for_microbit.tflite: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test int8 TFLite model properly --- #\n",
        "# --- Test for pruned, quantized or prunedANDquantized ---#\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the TFLite model\n",
        "tflite_model_path = 'simple_cnn_quantized.tflite'\n",
        "#tflite_model_path = 'simple_cnn_prunedANDquantized.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input details:\", input_details)\n",
        "print(\"Output details:\", output_details)\n",
        "\n",
        "# Prepare test data as NumPy arrays\n",
        "X_test_np = X_test.numpy() if isinstance(X_test, torch.Tensor) else X_test\n",
        "y_test_np = y_test.numpy() if isinstance(y_test, torch.Tensor) else y_test\n",
        "\n",
        "# Get input quantization parameters\n",
        "input_scale, input_zero_point = input_details[0]['quantization']\n",
        "output_scale, output_zero_point = output_details[0]['quantization']\n",
        "\n",
        "# Accuracy tracking\n",
        "correct_preds = 0\n",
        "\n",
        "for i in range(len(X_test_np)):\n",
        "    # 1. Take one sample\n",
        "    sample = X_test_np[i]  # shape: [window_size, features]\n",
        "\n",
        "    # 2. Transpose to [features, window_size]\n",
        "    sample_transposed = np.transpose(sample, (1, 0))\n",
        "\n",
        "    # 3. Add batch dimension\n",
        "    sample_batch = np.expand_dims(sample_transposed, axis=0).astype(np.float32)\n",
        "\n",
        "    # 4. Quantize input\n",
        "    input_data = np.round(sample_batch / input_scale + input_zero_point).astype(np.int8)\n",
        "\n",
        "    # 5. Set input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    # 6. Invoke the interpreter\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # 7. Get output and dequantize\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    output_data = output_scale * (output_data.astype(np.float32) - output_zero_point)\n",
        "\n",
        "    # 8. Predicted class\n",
        "    predicted_class = np.argmax(output_data)\n",
        "\n",
        "    # 9. Compare with true label\n",
        "    if predicted_class == y_test_np[i]:\n",
        "        correct_preds += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_preds / len(X_test_np)\n",
        "print(f\"Test Accuracy of TFLite int8 model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "2RrjQAnAQ5j_",
        "outputId": "dec283af-cdc2-4b1f-89c8-e856b478053e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input details: [{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1,   3, 100], dtype=int32), 'shape_signature': array([  1,   3, 100], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output details: [{'name': 'PartitionedCall:0', 'index': 23, 'shape': array([1, 6], dtype=int32), 'shape_signature': array([1, 6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2610560222.py:43: RuntimeWarning: divide by zero encountered in divide\n",
            "  input_data = np.round(sample_batch / input_scale + input_zero_point).astype(np.int8)\n",
            "/tmp/ipython-input-2610560222.py:43: RuntimeWarning: invalid value encountered in cast\n",
            "  input_data = np.round(sample_batch / input_scale + input_zero_point).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input:0 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2610560222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# 5. Set input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# 6. Invoke the interpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    762\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[0;32m--> 764\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input:0 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Dataset PAMAP2**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aB498o73z9yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove columns heartrate and temp of the three IMUs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(train_data_file)\n",
        "\n",
        "# Remove columns\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df = df.drop(df.columns[11], axis=1)\n",
        "df = df.drop(df.columns[20], axis=1)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# Print a message to confirm that the file was saved\n",
        "print(f\"Updated CSV saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "ybJnxdyB7hhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the activity names in the first column with their corresponding IDs\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the modified CSV\n",
        "modified_train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_columns_removed.csv'\n",
        "\n",
        "# Load the modified CSV into a DataFrame without assuming a header\n",
        "df = pd.read_csv(modified_train_data_file, header=None)\n",
        "\n",
        "# Print the first few rows to check the content\n",
        "print(\"Original DataFrame first few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Create a dictionary to map activity names to activity IDs\n",
        "activity_map = {\n",
        "    'lying': 1,\n",
        "    'sitting': 2,\n",
        "    'standing': 3,\n",
        "    'walking': 4,\n",
        "    'running': 5,\n",
        "    'cycling': 6,\n",
        "    'nordic_walking': 7,\n",
        "    'watching_TV': 9,\n",
        "    'computer_work': 10,\n",
        "    'car_driving': 11,\n",
        "    'ascending_stairs': 12,\n",
        "    'descending_stairs': 13,\n",
        "    'vacuum_cleaning': 16,\n",
        "    'ironing': 17,\n",
        "    'folding_laundry': 18,\n",
        "    'house_cleaning': 19,\n",
        "    'playing_soccer': 20,\n",
        "    'rope_jumping': 24,\n",
        "    'other': 0\n",
        "}\n",
        "\n",
        "# Replace the activity names in the first column with their corresponding IDs\n",
        "df.iloc[:, 0] = df.iloc[:, 0].map(activity_map)\n",
        "\n",
        "# Print the first few rows after the transformation to ensure it's working\n",
        "print(\"\\nDataFrame after replacing activity names with IDs:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv'\n",
        "df.to_csv(output_file, index=False, header=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"Updated CSV with activity IDs saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "zuGjD4IYE8mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split CSV in train (excluding user 5) and test (only user 5) csv files\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path for the CSV file containing activity IDs\n",
        "train_data_file_with_ids = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/pamap2_with_activityIDs.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame without assuming a header\n",
        "df = pd.read_csv(train_data_file_with_ids, header=None)\n",
        "\n",
        "# Print the first few rows to check the content\n",
        "print(\"Original DataFrame first few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Filter rows where the user ID is 5 and save them as test_pamap2.csv\n",
        "test_df = df[df.iloc[:, 1] == 5]\n",
        "test_df.to_csv('/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.csv', index=False, header=False)\n",
        "\n",
        "# Filter rows where the user ID is not 5 (excluding) and save them as train_pamap2.csv\n",
        "train_df = df[df.iloc[:, 1] != 5]\n",
        "train_df.to_csv('/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.csv', index=False, header=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Data has been split into train_pamap2.csv (excluding user ID 5) and test_pamap2.csv (user ID 5 only).\")\n"
      ],
      "metadata": {
        "id": "ColCz8mlHdJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert .csv to .dat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "train_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.csv'\n",
        "test_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.csv'\n",
        "\n",
        "# Function to convert rows into the required format and save as .dat\n",
        "def convert_to_dat(input_file, output_file):\n",
        "    # Load the CSV file without a header\n",
        "    df = pd.read_csv(input_file, header=None)\n",
        "\n",
        "    # Open the output .dat file to write\n",
        "    with open(output_file, 'w') as file:\n",
        "        for _, row in df.iterrows():\n",
        "            # Convert row into the required format: {{column 2, column 3, column 4, ...}, column 0}\n",
        "            row_data = \"{{\" + \",\".join(map(str, row[2:])) + \"},\" + str(int(row[0])) + \"}\\n\"\n",
        "            file.write(row_data)\n",
        "\n",
        "# Convert train_pamap2.csv to train_pamap2.dat\n",
        "convert_to_dat(train_file, '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.dat')\n",
        "\n",
        "# Convert test_pamap2.csv to test_pamap2.dat\n",
        "convert_to_dat(test_file, '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.dat')\n",
        "\n",
        "print(\"Conversion to .dat files is complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cncd9etsQcGQ",
        "outputId": "30a72bcb-8efa-4113-b9c8-d0768d1e4d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion to .dat files is complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time windows\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "# Function to create time windows\n",
        "def create_time_windows(data, labels, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i + window_size])  # Select window of data\n",
        "        y.append(labels[i + (window_size-1)])  # Label is from the last element of the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Updated data loading function\n",
        "def load_data(file_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Use regular expression to match the pattern {{feature_vector}, label}\n",
        "            match = re.match(r\"\\{\\{([0-9.,-]+)\\},\\s*(\\d+)\\}\", line.strip())\n",
        "\n",
        "            if match:\n",
        "                # Extract feature vector and label\n",
        "                feature_str = match.group(1)  # The feature string \"8.24,-2.11,3.87\"\n",
        "                label = int(match.group(2))  # The label \"4\"\n",
        "\n",
        "                # Convert the feature string to a list of floats\n",
        "                feature_vector = list(map(float, feature_str.split(',')))\n",
        "\n",
        "                features.append(feature_vector)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load train and test data\n",
        "train_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/train_pamap2.dat'  # Adjust path to your file\n",
        "test_data_file = '/content/drive/MyDrive/MyBox/prof/projetos de pesquisa/pauNaJaca/dataset/PAMAP2/test_pamap2.dat'  # Adjust path to your file\n",
        "\n",
        "print(\"File loaded:\")\n",
        "\n",
        "# Define the window size\n",
        "window_size = 10\n",
        "\n",
        "# Create time windows\n",
        "X_train, y_train = load_data(train_data_file)\n",
        "X_train, y_train = create_time_windows(X_train, y_train, window_size)\n",
        "\n",
        "X_test, y_test = load_data(test_data_file)\n",
        "X_test, y_test = create_time_windows(X_test, y_test, window_size)\n",
        "\n",
        "print(\"Time window created:\")\n",
        "\n",
        "# Print the first feature vector and label\n",
        "print(\"First feature vector in X_train:\")\n",
        "print(X_train[0])  # First row (first feature vector)\n",
        "print(\"Second feature vector in X_train:\")\n",
        "print(X_train[1])  # First row (first feature vector)\n",
        "\n",
        "print(\"First label in y_train:\")\n",
        "print(y_train[0])  # First label\n",
        "print(\"Second label in y_train:\")\n",
        "print(y_train[1])  # First label\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)  # For classification (long type for labels)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# # Print the first feature vector and label\n",
        "# print(\"First feature vector in X_train:\")\n",
        "# print(X_train[0])  # First row (first feature vector)\n",
        "# print(\"First label in y_train:\")\n",
        "# print(y_train[0])  # First label\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "2cslxq8BXNTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=27, out_channels=27, kernel_size=3, padding=1)  # input channels = number of features\n",
        "        self.conv2 = nn.Conv1d(in_channels=27, out_channels=16, kernel_size=3, padding=1)  # input channels = number of features\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the input size for the fully connected layer based on the output size of conv1\n",
        "        self.fc1 = nn.Linear(16 * (X_train.shape[1] // 2), 64)  # Flattened size after pooling\n",
        "        self.fc2 = nn.Linear(64, 25)  # Assuming 25 classes for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Add channel dimension (change shape to [batch_size, channels, seq_length])\n",
        "\n",
        "        x = torch.relu(self.conv1(x))  # Apply conv1 and pooling\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2 and pooling\n",
        "        #x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1 and pooling\n",
        "\n",
        "        x = x.view(-1, 16 * (x.shape[2]))  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Apply first fully connected layer\n",
        "        x = self.fc2(x)  # Output layer (no activation since we'll apply softmax in loss)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Loss function (cross-entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "O_gyZYu9XqYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "num_epochs = 3  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Track loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ufOIrAbaW1F",
        "outputId": "e93de07b-9973-41eb-b535-331cf2e3b713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.2979, Accuracy: 0.9041\n",
            "Epoch 2/3, Loss: 0.1936, Accuracy: 0.9378\n",
            "Epoch 3/3, Loss: 0.1731, Accuracy: 0.9447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct_preds = (predicted == y_test).sum().item()\n",
        "    accuracy = correct_preds / len(y_test)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44agAuPBeDNd",
        "outputId": "fb075590-cbbb-45ef-a68d-d152490c3aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7133\n"
          ]
        }
      ]
    }
  ]
}